{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganizzzm/huji_cources/blob/main/MyCopy_of_IMPR_Ex5_Deep_Style_Image_Prior_2022_2023_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important!\n",
        "When initially opening the notebook there should be a text to the right of the \"Help\" menu saying \"Changes will not be saved\".\n",
        "![WhatsApp Image 2021-01-02 at 22.02.20.jpeg](data:image/jpeg;base64,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)\n",
        "\n",
        "To ensure you can make changes to the notebook save a copy of it to your own drive and work on that one. You can do that by going to \"File\" -> \"Save a copy in Drive\".\n",
        "\n",
        "**Failing to do so will result in code loss!**\n",
        "\n",
        " **Note** Make sure you are the only one that has access to it!\n"
      ],
      "metadata": {
        "id": "7PbmNXuh7WuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #Image Processing - 67829. { display-mode: \"form\" }\n",
        "#@markdown ##Exercise 5:  Deep Style Image Prior\n",
        "#@markdown ##Due date: 26.01.2023 at 23:59\n",
        "#@title{ display-mode: \"form\" }\n",
        "\n",
        "#@markdown\n",
        "#@markdown This exercise is a bit different than the rest of the exercises in the course.\n",
        "#@markdown The submissions will be a PDF file with your answers and results to the exercise\n",
        "#@markdown as well as some files so that we can verify the authenticity of your results.\n",
        "#@markdown This notebook provides the basic code, but you do not need to adhere to some specific API\n",
        "#@markdown and we will not be running unit tests on your code.\n",
        "#@markdown We will however, be going over your code and running it manually.\n",
        "#@markdown Moreover, we will be running tests to ensure the authenticity of your solution and detect plagiarism\n",
        "#@markdown\n",
        "#@markdown\n",
        "#@markdown Before you start working on the exercise it is recommended that you review the lecture slides covering neural networks,\n",
        "#@markdown\n",
        "#@markdown\n",
        "#@markdown **NOTE**: Neural networks are typically trained on GPUs, without GPUs training takes much longer.\n",
        "#@markdown To enable GPU tranining click on \"Runtime\" -> \"Change runtime type\" -> \"GPU\" -> \"SAVE\".\n",
        "#@markdown\n",
        "#@markdown **NOTE**: A short guide on debugging your code using colab is availble [here](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.06-Errors-and-Debugging.ipynb#scrollTo=qnIn-rWFqoww).\n",
        "\n",
        "#@markdown But first, we have to download all of the dependencies and install them.\n",
        "#@markdown Play this cell to download it and get everything ready. markdown This may take a few minutes.\n",
        "\n",
        "\n",
        "!mkdir impr_ex5_resources\n",
        "%cd impr_ex5_resources\n",
        "#!wget \"https://www.cs.huji.ac.il/~impr/shape_predictor_68_face_landmarks.dat\" -O shape_predictor_68_face_landmarks.dat\n",
        "#!wget \"https://www.cs.huji.ac.il/~impr/align_faces.py\" -O align_faces.py\n",
        "!wget \"https://www.cs.huji.ac.il/~impr/stylegan2-ada-pytorch.tar\" -O stylegan2-ada-pytorch.tar\n",
        "!tar -xvf stylegan2-ada-pytorch.tar\n",
        "!rm -f stylegan2-ada-pytorch.tar\n",
        "\n",
        "import sys\n",
        "ROOT_PATH=\"/content/impr_ex5_resources/stylegan2-ada-pytorch\"\n",
        "sys.path.append(ROOT_PATH)\n",
        "\n",
        "\n",
        "!pip install ninja\n",
        "!pip install mediapy\n",
        "CHECKPOINTS_PATH = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
        "\n",
        "\n",
        "\n",
        "import copy\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from time import perf_counter\n",
        "import click\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import dnnlib\n",
        "import legacy\n",
        "import numpy as np\n",
        "from skimage.draw import line\n",
        "from torch.nn.functional import conv2d,conv1d\n",
        "import mediapy as media\n",
        "from IPython.display import clear_output\n"
      ],
      "metadata": {
        "id": "Jean9HbQ7iq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0eb2d2c-7b86-485b-fb0e-c516f3f32dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/impr_ex5_resources/impr_ex5_resources\n",
            "--2023-01-16 09:45:43--  https://www.cs.huji.ac.il/~impr/stylegan2-ada-pytorch.tar\n",
            "Resolving www.cs.huji.ac.il (www.cs.huji.ac.il)... 132.65.118.16\n",
            "Connecting to www.cs.huji.ac.il (www.cs.huji.ac.il)|132.65.118.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://www.cs.huji.ac.il/w~impr/stylegan2-ada-pytorch.tar [following]\n",
            "--2023-01-16 09:45:44--  https://www.cs.huji.ac.il/w~impr/stylegan2-ada-pytorch.tar\n",
            "Reusing existing connection to www.cs.huji.ac.il:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100364800 (96M) [application/x-tar]\n",
            "Saving to: ‘stylegan2-ada-pytorch.tar’\n",
            "\n",
            "stylegan2-ada-pytor 100%[===================>]  95.71M  15.5MB/s    in 7.4s    \n",
            "\n",
            "2023-01-16 09:45:52 (13.0 MB/s) - ‘stylegan2-ada-pytorch.tar’ saved [100364800/100364800]\n",
            "\n",
            "stylegan2-ada-pytorch/\n",
            "stylegan2-ada-pytorch/metrics/\n",
            "stylegan2-ada-pytorch/._dataset_tool.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/dataset_tool.py\n",
            "stylegan2-ada-pytorch/._.DS_Store\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.FinderInfo'\n",
            "stylegan2-ada-pytorch/.DS_Store\n",
            "stylegan2-ada-pytorch/._style_mixing.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/style_mixing.py\n",
            "stylegan2-ada-pytorch/torch_utils/\n",
            "stylegan2-ada-pytorch/._legacy.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/legacy.py\n",
            "stylegan2-ada-pytorch/._generate.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/generate.py\n",
            "stylegan2-ada-pytorch/training/\n",
            "stylegan2-ada-pytorch/._README.md\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/README.md\n",
            "stylegan2-ada-pytorch/._projector.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/projector.py\n",
            "stylegan2-ada-pytorch/._train.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/train.py\n",
            "stylegan2-ada-pytorch/._calc_metrics.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/calc_metrics.py\n",
            "stylegan2-ada-pytorch/dnnlib/\n",
            "stylegan2-ada-pytorch/align_faces/\n",
            "stylegan2-ada-pytorch/._LICENSE.txt\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/LICENSE.txt\n",
            "stylegan2-ada-pytorch/align_faces/._shape_predictor_68_face_landmarks.dat\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/align_faces/shape_predictor_68_face_landmarks.dat\n",
            "stylegan2-ada-pytorch/align_faces/._align_faces.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/align_faces/align_faces.py\n",
            "stylegan2-ada-pytorch/dnnlib/._util.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/dnnlib/util.py\n",
            "stylegan2-ada-pytorch/dnnlib/.___init__.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/dnnlib/__init__.py\n",
            "stylegan2-ada-pytorch/dnnlib/__pycache__/\n",
            "stylegan2-ada-pytorch/dnnlib/__pycache__/._util.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/dnnlib/__pycache__/util.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/dnnlib/__pycache__/.___init__.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/dnnlib/__pycache__/__init__.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/training/.___init__.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/__init__.py\n",
            "stylegan2-ada-pytorch/training/__pycache__/\n",
            "stylegan2-ada-pytorch/training/._augment.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/augment.py\n",
            "stylegan2-ada-pytorch/training/._training_loop.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/training_loop.py\n",
            "stylegan2-ada-pytorch/training/._dataset.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/dataset.py\n",
            "stylegan2-ada-pytorch/training/._networks.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/networks.py\n",
            "stylegan2-ada-pytorch/training/._loss.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/loss.py\n",
            "stylegan2-ada-pytorch/training/__pycache__/._networks.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/__pycache__/networks.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/training/__pycache__/.___init__.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/training/__pycache__/__init__.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/._misc.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/misc.py\n",
            "stylegan2-ada-pytorch/torch_utils/._persistence.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/persistence.py\n",
            "stylegan2-ada-pytorch/torch_utils/._custom_ops.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/custom_ops.py\n",
            "stylegan2-ada-pytorch/torch_utils/.___init__.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/__init__.py\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/\n",
            "stylegan2-ada-pytorch/torch_utils/._training_stats.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/training_stats.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._bias_act.cpp\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/bias_act.cpp\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._upfirdn2d.cpp\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/upfirdn2d.cpp\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._grid_sample_gradfix.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/grid_sample_gradfix.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._bias_act.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/bias_act.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._upfirdn2d.cu\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/upfirdn2d.cu\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._bias_act.cu\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/bias_act.cu\n",
            "stylegan2-ada-pytorch/torch_utils/ops/.___init__.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__init__.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._upfirdn2d.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/upfirdn2d.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._bias_act.h\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/bias_act.h\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._fma.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/fma.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._conv2d_resample.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/conv2d_resample.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._upfirdn2d.h\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/upfirdn2d.h\n",
            "stylegan2-ada-pytorch/torch_utils/ops/._conv2d_gradfix.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/conv2d_gradfix.py\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/._fma.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/fma.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/._conv2d_gradfix.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/conv2d_gradfix.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/._bias_act.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/bias_act.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/._upfirdn2d.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/upfirdn2d.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/._conv2d_resample.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/conv2d_resample.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/.___init__.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/ops/__pycache__/__init__.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/._persistence.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/persistence.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/._custom_ops.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/custom_ops.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/.___init__.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/__init__.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/._misc.cpython-37.pyc\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/torch_utils/__pycache__/misc.cpython-37.pyc\n",
            "stylegan2-ada-pytorch/metrics/._precision_recall.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/precision_recall.py\n",
            "stylegan2-ada-pytorch/metrics/._frechet_inception_distance.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/frechet_inception_distance.py\n",
            "stylegan2-ada-pytorch/metrics/.___init__.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/__init__.py\n",
            "stylegan2-ada-pytorch/metrics/._metric_utils.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/metric_utils.py\n",
            "stylegan2-ada-pytorch/metrics/._metric_main.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/metric_main.py\n",
            "stylegan2-ada-pytorch/metrics/._perceptual_path_length.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/perceptual_path_length.py\n",
            "stylegan2-ada-pytorch/metrics/._inception_score.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/inception_score.py\n",
            "stylegan2-ada-pytorch/metrics/._kernel_inception_distance.py\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "stylegan2-ada-pytorch/metrics/kernel_inception_distance.py\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (1.11.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mediapy in /usr/local/lib/python3.8/dist-packages (1.1.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mediapy) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapy) (1.21.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from mediapy) (7.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapy) (3.2.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->mediapy) (2.0.10)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapy) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapy) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapy) (0.11.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->mediapy) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->mediapy) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->mediapy) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->mediapy) (0.7.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e9702da960d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdtd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0meurosat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEuroSAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfakedata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfer2013\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFER2013\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfgvc_aircraft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFGVCAircraft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/fakedata.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautoaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_interpolation_modes_from_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_pil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maccimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pil_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/_pil_constants.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mNEAREST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mAFFINE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAFFINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mFLIP_LEFT_RIGHT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLIP_LEFT_RIGHT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mFLIP_TOP_BOTTOM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLIP_TOP_BOTTOM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0m_raise_version_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"module '{}' has no attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'Transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive"
      ],
      "metadata": {
        "id": "LY3SOM7bKggZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **NOTE**: It is strongly advised you save your results to Google\n",
        "#@markdown Drive as they will be deleted from Colab once it restarts.\n",
        "#@markdown To connect Google Drive run this cell.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E8dlOxYdKhyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531b0e55-2298-4972-fc89-776e06aa54f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the root dir of your Google Drive. To choose the destenation of the dir to save and read from, create it in your Google Drive and add the relative path to the \"GDRIVE_SAVE_REL_PATH\" variable below."
      ],
      "metadata": {
        "id": "Lp4YuqH2Lmku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_GDRIVE_PATH=\"/content/gdrive/MyDrive/\"\n",
        "GDRIVE_SAVE_REL_PATH = \"im5/\"\n",
        "FULL_GDRIVE_SAVE_PATH = ROOT_GDRIVE_PATH + GDRIVE_SAVE_REL_PATH"
      ],
      "metadata": {
        "id": "LAjRCCKLLBI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Variables"
      ],
      "metadata": {
        "id": "mNEXRTie-E93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GAUSSIAN_BLUR_DEGRADATION= 'GAUSSIAN_BLUR_DEGRADATION'\n",
        "GRAYSCALE_DEGRADATION = 'GRAYSCALE_DEGRADATION'\n",
        "INPAINTING_DEGRADATION = 'INPAINTING_DEGRADATION'\n",
        "NO_DEGRADATION= 'NO_DEGRADATION'\n"
      ],
      "metadata": {
        "id": "RFt1dKcO95Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Alignment"
      ],
      "metadata": {
        "id": "NlMqcLzq9lHv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO2O4E5oXn-5"
      },
      "source": [
        "# The align_faces.py script takes in an input image path, an output image path, and a dat file path. The dat file is already downloaded for you, so leave it as it is.\n",
        "# It is advised that you save the files to google drive as restarting Colab will erase them.\n",
        "!python \"$ROOT_PATH/align_faces/align_faces.py\" 'INPUT_IMAGE_PATH_AND_NAME' 'OUTPUT_IMAGE_PATH_AND_NAME' \"$ROOT_PATH/align_faces/shape_predictor_68_face_landmarks.dat\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Degradation Functions"
      ],
      "metadata": {
        "id": "Z7GO7LxPC19b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # ********************************************************************************************************\n",
        "    # ******************                   NEED TO ADD DEGRADATION FUNCTIONS                ******************\n",
        "    # ********************************************************************************************************\n",
        "    def add_gaussian_noise(synth_images):\n",
        "      transform = transforms.GaussianBlur(kernel_size=(33, 33), sigma=(2, 21.0))\n",
        "      return transform(synth_images)\n",
        "\n",
        "    def add_colorization(synth_images):\n",
        "      gray_scale = (transforms.rgb_to_grayscale(synth_images, 3)).float()\n",
        "      return ans(grayscale)\n",
        "\n",
        "\n",
        "    def add_inpainting(synth_images):\n",
        "      transform = transforms.RandomErasing()\n",
        "      return transform(synth_images)"
      ],
      "metadata": {
        "id": "PY7gxd63zqnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Inversion"
      ],
      "metadata": {
        "id": "j2EbeYsb9u7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_latent_optimization(\n",
        "    loss_arr,\n",
        "    outdir,\n",
        "    degradation_mode,\n",
        "    G,\n",
        "    imgs_to_disply_dict,\n",
        "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "    *,\n",
        "    num_steps                  = 1000,\n",
        "    w_avg_samples              = 10000,\n",
        "    initial_learning_rate      = 0.1,\n",
        "    initial_noise_factor       = 0.05,\n",
        "    lr_rampdown_length         = 0.25,\n",
        "    lr_rampup_length           = 0.05,\n",
        "    noise_ramp_length          = 0.75,\n",
        "    regularize_noise_weight    = 1e5,\n",
        "    latent_dist_reg_weight     = 0.001,\n",
        "    device: torch.device\n",
        "\n",
        "):\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    print(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples.cpu().numpy().astype(np.float32)\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 18, C]\n",
        "    w_avg_original = torch.from_numpy(w_avg).to(device).float()\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "\n",
        "    # Load VGG16 feature detector.\n",
        "    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "    with dnnlib.util.open_url(url) as f:\n",
        "        vgg16 = torch.jit.load(f).eval().to(device)\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True)\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth image from opt_w\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = w_opt + w_noise\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        # Prep to save synth image\n",
        "        synth_image_save = (synth_images + 1) * (255/2)\n",
        "        synth_image_save = synth_image_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "\n",
        "\n",
        "        # ********************************************************************************************************\n",
        "        # ******************                   NEED TO FILL IN THE FOLLOWING CODE               ******************\n",
        "        # ********************************************************************************************************\n",
        "        if  degradation_mode == INPAINTING_DEGRADATION:\n",
        "          synth_images = add_inpainting(synth_images)\n",
        "        elif degradation_mode == GRAYSCALE_DEGRADATION:\n",
        "          synth_images = add_colorization(synth_images)\n",
        "        elif degradation_mode == GAUSSIAN_BLUR_DEGRADATION:\n",
        "          synth_images = add_gaussian_noise(synth_images)\n",
        "\n",
        "        # ********************************************************************************************************\n",
        "        # ******************                          END CODE TO ADD SECTION                   ******************\n",
        "        # ********************************************************************************************************\n",
        "\n",
        "\n",
        "        # Prep to save and show images\n",
        "        synth_image_degraded_save = (synth_images + 1) * (255/2)\n",
        "        synth_image_degraded_save = synth_image_degraded_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "\n",
        "\n",
        "        if step % 20 == 0:\n",
        "          imgs_to_disply_dict[\"Generated Image\"]=synth_image_save\n",
        "          imgs_to_disply_dict[\"Generated Degraded Image\"]=synth_image_degraded_save\n",
        "          clear_output(wait=True)\n",
        "          media.show_images(imgs_to_disply_dict,height=256)\n",
        "        if step % 100 == 0:\n",
        "          PIL.Image.fromarray(synth_image_save, 'RGB').save(f'{outdir}/intermidiate_%d_not_degraded.png'%step)\n",
        "          PIL.Image.fromarray(synth_image_degraded_save, 'RGB').save(f'{outdir}/intermidiate_%d_degraded.png'%step)\n",
        "\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255/2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "        # Compute loss\n",
        "        percep_loss = (target_features - synth_features).square().sum()\n",
        "        latent_dist_reg = F.l1_loss(w_avg_original, w_opt)\n",
        "        loss = percep_loss + reg_loss * regularize_noise_weight  + latent_dist_reg_weight * latent_dist_reg\n",
        "\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'step {step+1:>4d}/{num_steps}: percep_loss {percep_loss:<4.2f} latent_dist_reg {latent_dist_reg:<4.2f} loss {float(loss):<5.2f}' )\n",
        "        loss_arr.append(float(loss))\n",
        "        # Save inverted latent for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "    return w_out"
      ],
      "metadata": {
        "id": "daYIThcfi9PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdFYp5sBor2U"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def invert_image(degradation_mode,\n",
        "                   target_fname,\n",
        "                   outdir,\n",
        "                   seed=303,\n",
        "                   num_steps=1000,\n",
        "                   latent_dist_reg_weight=0.001):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load networks.\n",
        "    print('Loading networks from \"%s\"...' % CHECKPOINTS_PATH)\n",
        "    device = torch.device('cuda')\n",
        "    with dnnlib.util.open_url(CHECKPOINTS_PATH) as fp:\n",
        "        networks = legacy.load_network_pkl(fp)\n",
        "        G = networks['G_ema'].requires_grad_(False).to(device)\n",
        "\n",
        "\n",
        "    # Load target image.\n",
        "    if not os.path.exists(outdir):\n",
        "      os.makedirs(outdir)\n",
        "    target_pil = PIL.Image.open(target_fname).convert('RGB')\n",
        "    w, h = target_pil.size\n",
        "    s = min(w, h)\n",
        "    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
        "    target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
        "    target = torch.tensor([target_uint8.transpose([2, 0, 1])], device=device)\n",
        "    target_images = target[0].unsqueeze(0).to(device).to(torch.float32)\n",
        "\n",
        "    # ********************************************************************************************************\n",
        "    # ******************                   NEED TO FILL IN THE FOLLOWING CODE               ******************\n",
        "    # ********************************************************************************************************\n",
        "    if  degradation_mode == INPAINTING_DEGRADATION:\n",
        "       pass\n",
        "    elif degradation_mode == GRAYSCALE_DEGRADATION:\n",
        "      pass\n",
        "    elif degradation_mode == GAUSSIAN_BLUR_DEGRADATION:\n",
        "      target = add_gaussian_noise(target)\n",
        "\n",
        "    # ********************************************************************************************************\n",
        "    # ******************                          END CODE TO ADD SECTION                   ******************\n",
        "    # ********************************************************************************************************\n",
        "\n",
        "    #Save target image\n",
        "    target_to_save = target.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "    PIL.Image.fromarray(target_to_save, 'RGB').save(f'{outdir}/original_degraded_image.png')\n",
        "    imgs_to_disply_dict = {\n",
        "        \"Original Image\":target_uint8,\n",
        "        \"Original Degraded Image\":target_to_save,\n",
        "              }\n",
        "\n",
        "    # Run latent optimization\n",
        "    loss_arr = []\n",
        "    start_time = perf_counter()\n",
        "    optimization_steps = run_latent_optimization(\n",
        "        loss_arr,\n",
        "        outdir,\n",
        "        degradation_mode,\n",
        "        G,\n",
        "        imgs_to_disply_dict,\n",
        "        target[0],\n",
        "        num_steps=num_steps,\n",
        "        device=device,\n",
        "        latent_dist_reg_weight=latent_dist_reg_weight)\n",
        "\n",
        "    print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Save final inverted image and latent vector.\n",
        "    inverted_latent = optimization_steps[-1]\n",
        "    synth_image = G.synthesis(inverted_latent.unsqueeze(0), noise_mode='const')\n",
        "    synth_image = (synth_image + 1) * (255/2)\n",
        "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "    PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/final_inverted_image.png')\n",
        "    np.savez(f'{outdir}/inverted_latent.npz', latent=inverted_latent.unsqueeze(0).cpu().numpy())\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invert_image(GRAYSCALE_DEGRADATION, \"/content/gdrive/MyDrive/im5/alan.png\", \"/content/gdrive/MyDrive/im5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "xvB3jfsj-xya",
        "outputId": "9653a12f-2911-445a-f666-ce5bd7d2fa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"...\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl ... done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-babe981f0764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minvert_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRAYSCALE_DEGRADATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/gdrive/MyDrive/im5/alan.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/gdrive/MyDrive/im5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d6eb835a982e>\u001b[0m in \u001b[0;36minvert_image\u001b[0;34m(degradation_mode, target_fname, outdir, seed, num_steps, latent_dist_reg_weight)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINTS_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnetworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G_ema'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    984\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    }
  ]
}